{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#OPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\", \"API_KEY\")\n",
    "\n",
    "def short_specific_prompt(state_before, state_after, action_name, env_description=None):\n",
    "    return (\n",
    "        f\"Provide a short and precise analysis of why the agent failed in the Frozen Lake environment.\\n\"\n",
    "        f\"Map details: S = Start, F = Frozen, H = Hole, G = Goal, X = Agentâ€™s position\\n\\n\"\n",
    "        f\"Second to last state:\\n{state_before}\\n\\n\"\n",
    "        f\"Last action: {action_name}\\n\\n\"\n",
    "        f\"Last state:\\n{state_after}\\n\\n\"\n",
    "        f\"Please include specific factors that contributed to this failure.\"\n",
    "    )\n",
    "\n",
    "def long_detailed_prompt(state_before, state_after, action_name, env_description):\n",
    "    return (\n",
    "        f\"Provide a detailed analysis of the agent's failure in the Frozen Lake environment.\\n\"\n",
    "        f\"Environment details:\\n{env_description}\\n\\n\"\n",
    "        f\"Second to last state:\\n{state_before}\\n\\n\"\n",
    "        f\"Last action: {action_name}\\n\\n\"\n",
    "        f\"Last state:\\n{state_after}\\n\\n\"\n",
    "        f\"Please include specific factors that contributed to this failure.\"\n",
    "    )\n",
    "\n",
    "def generate_explanation(model_id, prompt):\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    data = {\"model\": model_id, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.7, \"top_p\": 0.9}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=30)\n",
    "        if response.status_code != 200:\n",
    "            return f\"Error: HTTP {response.status_code} - {response.text[:200]}\"\n",
    "        return response.json()['choices'][0]['message']['content'].strip()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: Request failed: {e}\"\n",
    "\n",
    "def get_state_description(env, s):\n",
    "    grid = [[c.decode(\"utf-8\") if isinstance(c, bytes) else c for c in row] for row in env.unwrapped.desc.tolist()]\n",
    "    ncol = env.unwrapped.ncol\n",
    "    r = int(s) // ncol\n",
    "    c = int(s) % ncol\n",
    "    grid[r][c] = \"X\"\n",
    "    return \"\\n\".join(\"\".join(row) for row in grid)\n",
    "\n",
    "ACTION_MAP = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
    "\n",
    "def generate_reference(env, state, action):\n",
    "    desc = env.unwrapped.desc\n",
    "    row = int(state) // env.unwrapped.ncol\n",
    "    col = int(state) % env.unwrapped.ncol\n",
    "    tile = desc[row][col].decode(\"utf-8\") if isinstance(desc[row][col], bytes) else desc[row][col]\n",
    "    action_name = ACTION_MAP[int(action)]\n",
    "    if tile == 'H':\n",
    "        return f\"The agent fell into a hole after moving {action_name} from the state shown.\"\n",
    "    if tile == 'G':\n",
    "        return f\"The agent reached the goal after moving {action_name} from the state shown.\"\n",
    "    return f\"The agent moved {action_name} from the state shown, but did not reach the goal.\"\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, device=\"cpu\")\n",
    "model.learn(total_timesteps=100000)\n",
    "env_description = env.unwrapped.__doc__\n",
    "\n",
    "models = [\n",
    "    (\"openai/gpt-5-chat\", \"GPT-5\"),\n",
    "    (\"meta-llama/llama-3.2-11b-vision-instruct\", \"Llama-3\"),\n",
    "    (\"deepseek/deepseek-r1-0528\", \"DeepSeek\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "episodes = 100\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    current_state = int(obs)\n",
    "    transitions = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_raw, _ = model.predict(current_state, deterministic=False)\n",
    "        try:\n",
    "            action = int(action_raw)\n",
    "        except Exception:\n",
    "            action = int(np.asarray(action_raw).item())\n",
    "        new_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        new_state = int(new_obs)\n",
    "        transitions.append((current_state, action, new_state, float(reward), bool(terminated), bool(truncated)))\n",
    "        current_state = new_state\n",
    "        done = terminated or truncated\n",
    "\n",
    "    if transitions and transitions[-1][3] == 0 and (transitions[-1][4] or transitions[-1][5]):\n",
    "        if len(transitions) >= 2:\n",
    "            before_state_idx = transitions[-2][2]\n",
    "            last_state_idx = transitions[-1][2]\n",
    "            last_action = transitions[-1][1]\n",
    "            state_before_text = get_state_description(env, before_state_idx)\n",
    "            state_after_text = get_state_description(env, last_state_idx)\n",
    "            action_name = ACTION_MAP[last_action]\n",
    "            reference_text = generate_reference(env, last_state_idx, last_action)\n",
    "\n",
    "            for prompt_func, prompt_name in [\n",
    "                (short_specific_prompt, \"ShortSpecific\"),\n",
    "                (long_detailed_prompt, \"LongDetailed\")\n",
    "            ]:\n",
    "                if prompt_name == \"ShortSpecific\":\n",
    "                    prompt = prompt_func(state_before_text, state_after_text, action_name)\n",
    "                else:\n",
    "                    prompt = prompt_func(state_before_text, state_after_text, action_name, env_description)\n",
    "\n",
    "                for model_id, model_name in models:\n",
    "                    explanation = generate_explanation(model_id, prompt)\n",
    "                    results.append([\n",
    "                        ep + 1, model_name, prompt_name,\n",
    "                        prompt, reference_text,\n",
    "                        state_before_text, state_after_text, action_name,\n",
    "                        explanation\n",
    "                    ])\n",
    "\n",
    "with open(\"FL_LLM_Explanations.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        \"Episode\", \"LLM\", \"PromptType\",\n",
    "        \"Prompt\", \"Reference\",\n",
    "        \"SecondToLastStateMap\", \"LastStateMap\", \"LastAction\",\n",
    "        \"Explanation\"\n",
    "    ])\n",
    "    writer.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
