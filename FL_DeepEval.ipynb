{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import PromptAlignmentMetric, GEval\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"API_KEY\"\n",
    "\n",
    "df = pd.read_csv(\"FL_LLM_Explanations.csv\")\n",
    "evaluation_model = \"gpt-4o\"\n",
    "\n",
    "def make_geval(name, steps, params):\n",
    "    return GEval(name=name, evaluation_steps=steps, evaluation_params=params, model=evaluation_model)\n",
    "\n",
    "correctness_metric = make_geval(\"Correctness\",\n",
    "    [\"Does the output contain factual errors?\", \"Heavily penalize exclusion of necessary information.\"],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])\n",
    "\n",
    "helpfulness_metric = make_geval(\"Helpfulness\",\n",
    "    [\"Does it provide useful information for understanding the agent's failure?\", \"Does it assist the user in drawing conclusions?\"],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT])\n",
    "\n",
    "conciseness_metric = make_geval(\"Conciseness\",\n",
    "    [\"Is it concise without losing meaning?\", \"Is there repetition?\"],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT])\n",
    "\n",
    "relevance_metric = make_geval(\"Relevance\",\n",
    "    [\"Does the output focus on the agentâ€™s failure context?\", \"Does it avoid unrelated responses?\"],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT])\n",
    "\n",
    "coherence_metric = make_geval(\"Coherence\",\n",
    "    [\"Is the response logically structured?\", \"Are sentences grammatically correct?\"],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT])\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    prompt = row[\"Prompt\"]\n",
    "    output = str(row[\"Explanation\"]).strip()\n",
    "    reference = row.get(\"Reference\", \"\")\n",
    "\n",
    "    test_case = LLMTestCase(input=prompt, actual_output=output, expected_output=reference)\n",
    "\n",
    "    evaluation_result = evaluate(\n",
    "        [test_case],\n",
    "        [\n",
    "            PromptAlignmentMetric(prompt_instructions=[prompt], model=evaluation_model, include_reason=True),\n",
    "            correctness_metric, helpfulness_metric, conciseness_metric,\n",
    "            relevance_metric, coherence_metric\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    metrics_data = evaluation_result.test_results[0].metrics_data\n",
    "\n",
    "    results.append({\n",
    "        \"Episode\": row[\"Episode\"],\n",
    "        \"PromptType\": row[\"PromptType\"],\n",
    "        \"Prompt\": prompt,\n",
    "        \"Reference\": reference,  \n",
    "        \"SecondToLastStateMap\": row[\"SecondToLastStateMap\"],\n",
    "        \"LastStateMap\": row[\"LastStateMap\"],\n",
    "        \"LastAction\": row[\"LastAction\"],\n",
    "        \"LLM\": row[\"LLM\"],  \n",
    "        \"Explanation\": output,  \n",
    "        \"Prompt Alignment Score\": metrics_data[0].score,\n",
    "        \"Prompt Alignment Reason\": metrics_data[0].reason,\n",
    "        \"Correctness Score\": metrics_data[1].score,\n",
    "        \"Correctness Reason\": metrics_data[1].reason,\n",
    "        \"Helpfulness Score\": metrics_data[2].score,\n",
    "        \"Helpfulness Reason\": metrics_data[2].reason,\n",
    "        \"Conciseness Score\": metrics_data[3].score,\n",
    "        \"Conciseness Reason\": metrics_data[3].reason,\n",
    "        \"Relevance Score\": metrics_data[4].score,\n",
    "        \"Relevance Reason\": metrics_data[4].reason,\n",
    "        \"Coherence Score\": metrics_data[5].score,\n",
    "        \"Coherence Reason\": metrics_data[5].reason\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "\n",
    "desired_order = [\n",
    "    \"Episode\", \"PromptType\", \"Prompt\", \"Reference\",\n",
    "    \"SecondToLastStateMap\", \"LastStateMap\", \"LastAction\",\n",
    "    \"LLM\", \"Explanation\",\n",
    "    \"Prompt Alignment Score\", \"Prompt Alignment Reason\",\n",
    "    \"Correctness Score\", \"Correctness Reason\",\n",
    "    \"Helpfulness Score\", \"Helpfulness Reason\",\n",
    "    \"Conciseness Score\", \"Conciseness Reason\",\n",
    "    \"Relevance Score\", \"Relevance Reason\",\n",
    "    \"Coherence Score\", \"Coherence Reason\"\n",
    "]\n",
    "eval_df = eval_df[desired_order]\n",
    "\n",
    "eval_df.to_csv(\"FL_DeepEval.csv\", index=False)\n",
    "\n",
    "print(eval_df.groupby([\"PromptType\", \"LLM\"])[[\n",
    "    \"Prompt Alignment Score\", \"Correctness Score\", \"Helpfulness Score\",\n",
    "    \"Conciseness Score\", \"Relevance Score\", \"Coherence Score\"\n",
    "]].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
