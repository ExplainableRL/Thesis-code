{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import PromptAlignmentMetric, GEval\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"API_KEY\"\n",
    "\n",
    "df = pd.read_csv(\"LL_LLM_Explanations.csv\")\n",
    "evaluation_model = \"gpt-4o\"\n",
    "\n",
    "def make_geval(name, steps, params):\n",
    "    return GEval(\n",
    "        name=name,\n",
    "        evaluation_steps=steps,\n",
    "        evaluation_params=params,\n",
    "        model=evaluation_model\n",
    "    )\n",
    "\n",
    "correctness_metric = make_geval(\n",
    "    \"Correctness\",\n",
    "    [\n",
    "        \"Does the explanation correctly describe why the agent failed?\",\n",
    "        \"Does it include the correct cause of failure based on velocity, angle, and contact states?\",\n",
    "        \"Heavily penalize factual errors or missing necessary details.\"\n",
    "    ],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT]\n",
    ")\n",
    "\n",
    "helpfulness_metric = make_geval(\n",
    "    \"Helpfulness\",\n",
    "    [\n",
    "        \"Does the explanation provide insights that would help someone understand why the agent failed?\",\n",
    "        \"Does it mention key contributing factors clearly?\"\n",
    "    ],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "conciseness_metric = make_geval(\n",
    "    \"Conciseness\",\n",
    "    [\n",
    "        \"Is the explanation concise but still informative?\",\n",
    "        \"Does it avoid redundancy or unnecessary elaboration?\"\n",
    "    ],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "relevance_metric = make_geval(\n",
    "    \"Relevance\",\n",
    "    [\n",
    "        \"Is the explanation focused on the failure and not on unrelated aspects?\",\n",
    "        \"Does it directly address the failure context?\"\n",
    "    ],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "coherence_metric = make_geval(\n",
    "    \"Coherence\",\n",
    "    [\n",
    "        \"Is the explanation logically structured and grammatically sound?\",\n",
    "        \"Are cause-and-effect relationships clear?\"\n",
    "    ],\n",
    "    [LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    prompt = row[\"Prompt\"]\n",
    "    output = str(row[\"Explanation\"]).strip()\n",
    "    reference = str(row[\"Reference\"]).strip()\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=prompt,\n",
    "        actual_output=output,\n",
    "        expected_output=reference\n",
    "    )\n",
    "\n",
    "    evaluation_result = evaluate(\n",
    "        [test_case],\n",
    "        [\n",
    "            PromptAlignmentMetric(prompt_instructions=[prompt], model=evaluation_model, include_reason=True),\n",
    "            correctness_metric,\n",
    "            helpfulness_metric,\n",
    "            conciseness_metric,\n",
    "            relevance_metric,\n",
    "            coherence_metric\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    m = evaluation_result.test_results[0].metrics_data\n",
    "\n",
    "    results.append({\n",
    "        \"Episode\": row[\"Episode\"],\n",
    "        \"PromptType\": row[\"PromptType\"],\n",
    "        \"Prompt\": prompt,\n",
    "        \"Reference\": reference,\n",
    "        \"LLM\": row[\"LLM\"],\n",
    "        \"Explanation\": output,\n",
    "        \"Prompt Alignment Score\": m[0].score,\n",
    "        \"Prompt Alignment Reason\": m[0].reason,\n",
    "        \"Correctness Score\": m[1].score,\n",
    "        \"Correctness Reason\": m[1].reason,\n",
    "        \"Helpfulness Score\": m[2].score,\n",
    "        \"Helpfulness Reason\": m[2].reason,\n",
    "        \"Conciseness Score\": m[3].score,\n",
    "        \"Conciseness Reason\": m[3].reason,\n",
    "        \"Relevance Score\": m[4].score,\n",
    "        \"Relevance Reason\": m[4].reason,\n",
    "        \"Coherence Score\": m[5].score,\n",
    "        \"Coherence Reason\": m[5].reason\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "\n",
    "cols = [\n",
    "    \"Episode\", \"PromptType\", \"Prompt\", \"Reference\", \"LLM\", \"Explanation\",\n",
    "    \"Prompt Alignment Score\", \"Prompt Alignment Reason\",\n",
    "    \"Correctness Score\", \"Correctness Reason\",\n",
    "    \"Helpfulness Score\", \"Helpfulness Reason\",\n",
    "    \"Conciseness Score\", \"Conciseness Reason\",\n",
    "    \"Relevance Score\", \"Relevance Reason\",\n",
    "    \"Coherence Score\", \"Coherence Reason\"\n",
    "]\n",
    "\n",
    "eval_df = eval_df[cols]\n",
    "\n",
    "regular_df = eval_df[eval_df[\"PromptType\"] != \"ShortSpecificModified\"]\n",
    "modified_df = eval_df[eval_df[\"PromptType\"] == \"ShortSpecificModified\"]\n",
    "\n",
    "regular_df.to_csv(\"LL_DeepEval.csv\", index=False)\n",
    "modified_df.to_csv(\"Modified_LL_DeepEval.csv\", index=False)\n",
    "\n",
    "print(regular_df.groupby([\"PromptType\", \"LLM\"])[[\n",
    "    \"Prompt Alignment Score\", \"Correctness Score\", \"Helpfulness Score\",\n",
    "    \"Conciseness Score\", \"Relevance Score\", \"Coherence Score\"\n",
    "]].describe())\n",
    "\n",
    "print(modified_df.groupby([\"PromptType\", \"LLM\"])[[\n",
    "    \"Prompt Alignment Score\", \"Correctness Score\", \"Helpfulness Score\",\n",
    "    \"Conciseness Score\", \"Relevance Score\", \"Coherence Score\"\n",
    "]].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
