{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1765741249723,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "x-wkbkcEVDWh",
    "outputId": "0fd00dd1-27e4-4dd9-c297-f0200891c0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Environment             PromptType       LLM  \\\n",
      "                                                             \n",
      "0             Frozen Lake           LongDetailed  DeepSeek   \n",
      "1             Frozen Lake           LongDetailed     GPT-5   \n",
      "2             Frozen Lake           LongDetailed   Llama-3   \n",
      "3             Frozen Lake          ShortSpecific  DeepSeek   \n",
      "4             Frozen Lake          ShortSpecific     GPT-5   \n",
      "5             Frozen Lake          ShortSpecific   Llama-3   \n",
      "6            Lunar Lander           LongDetailed  DeepSeek   \n",
      "7            Lunar Lander           LongDetailed     GPT-5   \n",
      "8            Lunar Lander           LongDetailed   Llama-3   \n",
      "9            Lunar Lander          ShortSpecific  DeepSeek   \n",
      "10           Lunar Lander          ShortSpecific     GPT-5   \n",
      "11           Lunar Lander          ShortSpecific   Llama-3   \n",
      "12  Obscured Lunar Lander  ShortSpecificModified  DeepSeek   \n",
      "13  Obscured Lunar Lander  ShortSpecificModified     GPT-5   \n",
      "14  Obscured Lunar Lander  ShortSpecificModified   Llama-3   \n",
      "\n",
      "   Prompt Alignment Score           Correctness Score            \\\n",
      "                     mean       std              mean       std   \n",
      "0                0.531915  0.504375          0.706026  0.312837   \n",
      "1                0.319149  0.471186          0.765032  0.169089   \n",
      "2                0.489362  0.505291          0.297755  0.078068   \n",
      "3                0.872340  0.337318          0.797007  0.070012   \n",
      "4                1.000000  0.000000          0.791009  0.136492   \n",
      "5                0.829787  0.379883          0.436085  0.159499   \n",
      "6                0.795455  0.408032          0.884811  0.102615   \n",
      "7                0.750000  0.438019          0.901744  0.102257   \n",
      "8                0.090909  0.290803          0.321218  0.170321   \n",
      "9                0.931818  0.254972          0.891503  0.090914   \n",
      "10               0.977273  0.150756          0.937669  0.033150   \n",
      "11               0.386364  0.492545          0.286208  0.094496   \n",
      "12               1.000000  0.000000          0.897306  0.098262   \n",
      "13               1.000000  0.000000          0.897872  0.109231   \n",
      "14               0.243182  0.422835          0.235971  0.076472   \n",
      "\n",
      "   Helpfulness Score           Conciseness Score           Relevance Score  \\\n",
      "                mean       std              mean       std            mean   \n",
      "0           0.821534  0.319627          0.842491  0.050949        0.956658   \n",
      "1           0.985188  0.008433          0.807763  0.104831        0.999504   \n",
      "2           0.902588  0.188033          0.647131  0.122519        0.954230   \n",
      "3           0.973262  0.022402          0.833978  0.053247        0.999900   \n",
      "4           0.970304  0.015447          0.901718  0.020631        0.999017   \n",
      "5           0.927756  0.022709          0.793963  0.057032        0.998392   \n",
      "6           0.986212  0.072663          0.576693  0.158176        0.995192   \n",
      "7           0.994627  0.004219          0.792944  0.109033        0.998261   \n",
      "8           0.839273  0.337224          0.537582  0.237176        0.850500   \n",
      "9           0.995547  0.005334          0.908651  0.014118        0.999572   \n",
      "10          0.996993  0.002727          0.924804  0.016830        0.999898   \n",
      "11          0.912177  0.250033          0.762995  0.189140        0.918968   \n",
      "12          0.995465  0.004200          0.900487  0.014195        0.999199   \n",
      "13          0.994384  0.005081          0.916472  0.014147        0.999373   \n",
      "14          0.920815  0.205176          0.790512  0.160273        0.940019   \n",
      "\n",
      "             Coherence Score            \n",
      "         std            mean       std  \n",
      "0   0.085250        0.786491  0.379894  \n",
      "1   0.001041        0.992698  0.005302  \n",
      "2   0.195904        0.900356  0.185481  \n",
      "3   0.000396        0.989412  0.008302  \n",
      "4   0.001432        0.991685  0.005514  \n",
      "5   0.002171        0.959771  0.043207  \n",
      "6   0.028281        0.945453  0.095288  \n",
      "7   0.001675        0.957820  0.017473  \n",
      "8   0.341240        0.766763  0.309616  \n",
      "9   0.000817        0.954643  0.020259  \n",
      "10  0.000389        0.966082  0.018810  \n",
      "11  0.251166        0.841686  0.232071  \n",
      "12  0.001573        0.953814  0.017840  \n",
      "13  0.000982        0.952976  0.023349  \n",
      "14  0.207553        0.844626  0.191433  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fl = pd.read_csv(\"FL_DeepEval.csv\")\n",
    "ll = pd.read_csv(\"LL_DeepEval.csv\")\n",
    "oll = pd.read_csv(\"Obscured_LL_DeepEval.csv\")\n",
    "\n",
    "fl[\"Environment\"] = \"Frozen Lake\"\n",
    "ll[\"Environment\"] = \"Lunar Lander\"\n",
    "oll[\"Environment\"] = \"Obscured Lunar Lander\"\n",
    "\n",
    "df = pd.concat([fl, ll, oll], ignore_index=True)\n",
    "\n",
    "metrics = [\"Prompt Alignment Score\", \"Correctness Score\", \"Helpfulness Score\",\n",
    "           \"Conciseness Score\", \"Relevance Score\", \"Coherence Score\"]\n",
    "\n",
    "agg = df.groupby([\"Environment\", \"PromptType\", \"LLM\"])[metrics].agg([\"mean\",\"std\"]).reset_index()\n",
    "print(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15280,
     "status": "ok",
     "timestamp": 1765752181121,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "5Ee3hSrHgMu-",
    "outputId": "af053271-4516-4e23-ae13-3122f5b61ff2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3310067609.py:172: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/tmp/ipython-input-3310067609.py:172: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/tmp/ipython-input-3310067609.py:172: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/tmp/ipython-input-3310067609.py:172: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/tmp/ipython-input-3310067609.py:172: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n",
      "/tmp/ipython-input-3310067609.py:172: UserWarning: \n",
      "\n",
      "The `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n",
      "\n",
      "  sns.pointplot(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "paths = {\n",
    "    \"FrozenLake\": \"FL_DeepEval.csv\",\n",
    "    \"LunarLander\": \"LL_DeepEval.csv\",\n",
    "    \"LunarLanderObscured\": \"Obscured_LL_DeepEval.csv\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for env_name, filename in paths.items():\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df[\"Environment\"] = env_name\n",
    "        dfs.append(df)\n",
    "\n",
    "if len(dfs) == 0:\n",
    "    raise FileNotFoundError(\"No evaluation CSVs found.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df[\"PromptType\"] = df[\"PromptType\"].astype(str).str.strip()\n",
    "df[\"LLM\"] = df[\"LLM\"].astype(str).str.strip()\n",
    "\n",
    "metrics = [\n",
    "    \"Prompt Alignment Score\",\n",
    "    \"Correctness Score\",\n",
    "    \"Helpfulness Score\",\n",
    "    \"Conciseness Score\",\n",
    "    \"Relevance Score\",\n",
    "    \"Coherence Score\"\n",
    "]\n",
    "\n",
    "mean_df = df.groupby([\"Environment\", \"PromptType\", \"LLM\"])[metrics].mean().reset_index()\n",
    "count_df = df.groupby([\"Environment\", \"PromptType\", \"LLM\"]).size().reset_index(name=\"N\")\n",
    "\n",
    "melted = mean_df.melt(\n",
    "    id_vars=[\"Environment\", \"PromptType\", \"LLM\"],\n",
    "    value_vars=metrics,\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"MeanScore\"\n",
    ")\n",
    "\n",
    "\n",
    "for env in sorted(df[\"Environment\"].unique()):\n",
    "    sub = melted[melted[\"Environment\"] == env]\n",
    "\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=sub,\n",
    "        x=\"LLM\",\n",
    "        y=\"MeanScore\",\n",
    "        hue=\"PromptType\",\n",
    "        col=\"Metric\",\n",
    "        kind=\"bar\",\n",
    "        col_wrap=3,\n",
    "        height=3.8,\n",
    "        aspect=1\n",
    "    )\n",
    "\n",
    "    for ax in g.axes.flatten():\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Mean Score\")\n",
    "\n",
    "    g._legend.set_title(\"\")\n",
    "    g.fig.suptitle(f\"{env} — Mean Scores by LLM and Prompt Type\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/mean_scores_{env}.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    sns.boxplot(data=df, x=\"LLM\", y=metric, hue=\"PromptType\")\n",
    "    plt.title(f\"Distribution of {metric} by LLM and Prompt Type\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(title=\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/boxplot_{metric.replace(' ', '_')}.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "for env in sorted(df[\"Environment\"].unique()):\n",
    "    env_subset = mean_df[mean_df[\"Environment\"] == env]\n",
    "\n",
    "    for metric in metrics:\n",
    "        pivot = env_subset.pivot(index=\"LLM\", columns=\"PromptType\", values=metric)\n",
    "\n",
    "        if pivot.empty:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(6, max(2, 0.8 * pivot.shape[0])))\n",
    "        sns.heatmap(\n",
    "            pivot,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cbar_kws={\"label\": metric},\n",
    "            linewidths=0.5\n",
    "        )\n",
    "        plt.xlabel(\"Prompt Type\")\n",
    "        plt.ylabel(\"LLM\")\n",
    "        plt.title(f\"{env} — Mean {metric}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"figures/heatmap_{env}_{metric.replace(' ', '_')}.png\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "corr = df[metrics].corr()\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"vlag\", center=0)\n",
    "plt.title(\"Correlation Matrix of Evaluation Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/correlation_matrix.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "regular_envs = df[df[\"Environment\"] != \"LunarLanderObscured\"]\n",
    "\n",
    "mean_df_regular = mean_df[mean_df[\"Environment\"] != \"LunarLanderObscured\"]\n",
    "\n",
    "pivot_multi = mean_df_regular.pivot_table(\n",
    "    index=[\"Environment\", \"LLM\"],\n",
    "    columns=\"PromptType\",\n",
    "    values=metrics\n",
    ")\n",
    "\n",
    "pivot_multi.columns = [f\"{metric}__{prompt}\" for metric, prompt in pivot_multi.columns]\n",
    "\n",
    "diff_df = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    col_long = f\"{metric}__LongDetailed\"\n",
    "    col_short = f\"{metric}__ShortSpecific\"\n",
    "    if col_long in pivot_multi.columns and col_short in pivot_multi.columns:\n",
    "        diff_df[metric] = pivot_multi[col_long] - pivot_multi[col_short]\n",
    "\n",
    "diff_df = pd.DataFrame(diff_df)\n",
    "\n",
    "if not diff_df.empty:\n",
    "    mean_gain = diff_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    mean_gain.plot(kind=\"bar\")\n",
    "    plt.title(\"Average difference of Long Detailed over Short Specific (Original Lunar + FrozenLake)\")\n",
    "    plt.ylabel(\"Mean Score Difference\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/mean_difference_bar.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "for env in sorted(df[\"Environment\"].unique()):\n",
    "    c = count_df[count_df[\"Environment\"] == env].pivot(\n",
    "        index=\"LLM\", columns=\"PromptType\", values=\"N\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    plt.figure(figsize=(6, max(2, 0.8 * c.shape[0])))\n",
    "    sns.heatmap(c, annot=True, fmt=\".0f\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Prompt Type\")\n",
    "    plt.ylabel(\"LLM\")\n",
    "    plt.title(f\"{env} — Example Counts per LLM and Prompt Type\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/counts_{env}.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.pointplot(\n",
    "        data=mean_df,\n",
    "        x=\"LLM\",\n",
    "        y=metric,\n",
    "        hue=\"PromptType\",\n",
    "        dodge=0.4,\n",
    "        join=False\n",
    "    )\n",
    "    plt.title(f\"Mean {metric} by LLM and Prompt Type\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(title=\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/pointplot_{metric.replace(' ', '_')}.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5483,
     "status": "ok",
     "timestamp": 1765744137608,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "KDi5T-68kt0h",
    "outputId": "db586f4e-17dd-47ab-f4a0-e22194b14348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched episodes: 44\n",
      "                   Metric      T-test p    Wilcoxon p   Cohen d  Mean diff  \\\n",
      "0  Prompt Alignment Score  1.084777e-03  1.287816e-03  0.203360   0.092424   \n",
      "1       Correctness Score  4.825080e-04  4.784445e-05 -0.217534  -0.026809   \n",
      "2       Helpfulness Score  2.324693e-01  2.792826e-03  0.073657   0.016083   \n",
      "3       Conciseness Score  1.649081e-16  1.429083e-16  0.542658   0.118545   \n",
      "4         Relevance Score  1.570890e-01  9.935268e-01  0.087332   0.019132   \n",
      "5         Coherence Score  3.530438e-01  3.389745e-02  0.057259   0.011731   \n",
      "\n",
      "   CI 95% low  CI 95% high  \n",
      "0    0.037600     0.147249  \n",
      "1   -0.041675    -0.011943  \n",
      "2   -0.010257     0.042423  \n",
      "3    0.092193     0.144897  \n",
      "4   -0.007295     0.045559  \n",
      "5   -0.012983     0.036444  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "from math import sqrt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "os.makedirs(\"figures/LL_comparison\", exist_ok=True)\n",
    "\n",
    "orig_path = \"LL_DeepEval.csv\"\n",
    "mod_path = \"Obscured_LL_DeepEval.csv\"\n",
    "\n",
    "orig = pd.read_csv(orig_path)\n",
    "mod = pd.read_csv(mod_path)\n",
    "\n",
    "orig[\"LLM\"] = orig[\"LLM\"].astype(str).str.strip()\n",
    "mod[\"LLM\"] = mod[\"LLM\"].astype(str).str.strip()\n",
    "\n",
    "orig = orig[orig[\"PromptType\"].isin([\"ShortSpecific\", \"LongDetailed\"])]\n",
    "\n",
    "mod = mod[mod[\"PromptType\"] == \"ShortSpecificModified\"]\n",
    "\n",
    "metrics = [\n",
    "    \"Prompt Alignment Score\",\n",
    "    \"Correctness Score\",\n",
    "    \"Helpfulness Score\",\n",
    "    \"Conciseness Score\",\n",
    "    \"Relevance Score\",\n",
    "    \"Coherence Score\"\n",
    "]\n",
    "\n",
    "matched = orig.merge(\n",
    "    mod,\n",
    "    on=[\"Episode\", \"LLM\"],\n",
    "    suffixes=(\"_orig\", \"_mod\"),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "if matched.empty:\n",
    "    raise ValueError(\"No overlapping episodes between LL and Obscured LL! Check seeds or file contents.\")\n",
    "\n",
    "print(f\"Matched episodes: {matched['Episode'].nunique()}\")\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    data = matched[[\"LLM\", f\"{metric}_orig\", f\"{metric}_mod\"]]\n",
    "    melted = pd.melt(\n",
    "        data,\n",
    "        id_vars=\"LLM\",\n",
    "        value_vars=[f\"{metric}_orig\", f\"{metric}_mod\"],\n",
    "        var_name=\"Version\",\n",
    "        value_name=\"Score\"\n",
    "    )\n",
    "    melted[\"Version\"] = melted[\"Version\"].map({\n",
    "        f\"{metric}_orig\": \"Original\",\n",
    "        f\"{metric}_mod\": \"Obscured\"\n",
    "    })\n",
    "\n",
    "    sns.barplot(data=melted, x=\"LLM\", y=\"Score\", hue=\"Version\")\n",
    "    plt.title(f\"{metric}: Original vs Obscured Lunar Lander\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/LL_comparison/{metric.replace(' ','_')}_bar.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "for metric in metrics:\n",
    "    matched[f\"{metric}_diff\"] = matched[f\"{metric}_mod\"] - matched[f\"{metric}_orig\"]\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(data=matched, x=\"LLM\", y=f\"{metric}_diff\")\n",
    "    plt.axhline(0, color=\"black\", linewidth=1)\n",
    "    plt.title(f\"Difference ({metric}): Obscured − Original\")\n",
    "    plt.ylabel(\"Score Difference\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/LL_comparison/{metric.replace(' ','_')}_difference.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.scatterplot(\n",
    "        x=matched[f\"{metric}_orig\"],\n",
    "        y=matched[f\"{metric}_mod\"],\n",
    "        hue=matched[\"LLM\"]\n",
    "    )\n",
    "    plt.plot([0,1],[0,1], \"--\", color=\"black\")\n",
    "    plt.xlabel(\"Original\")\n",
    "    plt.ylabel(\"Obscured\")\n",
    "    plt.title(f\"{metric}: Original vs Obscured (paired scatter)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/LL_comparison/{metric.replace(' ','_')}_scatter.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "stats_results = []\n",
    "\n",
    "def cohens_d(x, y):\n",
    "    return (np.mean(y) - np.mean(x)) / np.std(y - x, ddof=1)\n",
    "\n",
    "for metric in metrics:\n",
    "    x = matched[f\"{metric}_orig\"]\n",
    "    y = matched[f\"{metric}_mod\"]\n",
    "\n",
    "    t_stat, p_ttest = ttest_rel(y, x)\n",
    "\n",
    "    w_stat, p_wilcox = wilcoxon(y - x)\n",
    "\n",
    "    d = cohens_d(x, y)\n",
    "\n",
    "    diff = (y - x)\n",
    "    mean_diff = diff.mean()\n",
    "    ci_low = mean_diff - 1.96 * diff.std(ddof=1) / sqrt(len(diff))\n",
    "    ci_high = mean_diff + 1.96 * diff.std(ddof=1) / sqrt(len(diff))\n",
    "\n",
    "    stats_results.append({\n",
    "        \"Metric\": metric,\n",
    "        \"T-test p\": p_ttest,\n",
    "        \"Wilcoxon p\": p_wilcox,\n",
    "        \"Cohen d\": d,\n",
    "        \"Mean diff\": mean_diff,\n",
    "        \"CI 95% low\": ci_low,\n",
    "        \"CI 95% high\": ci_high\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats_results)\n",
    "stats_df.to_csv(\"figures/LL_comparison/LL_original_vs_obscured_statistics.csv\", index=False)\n",
    "\n",
    "print(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1765746734942,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "4m8VRhsjtJ_k",
    "outputId": "e2503043-07f4-489e-bcfc-721b6f04c7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Environment       LLM  Prompt Alignment  Correctness  Helpfulness  \\\n",
      "0   Frozen Lake  DeepSeek             -0.34        -0.09        -0.15   \n",
      "1   Frozen Lake     GPT-5             -0.68        -0.03         0.01   \n",
      "2   Frozen Lake   Llama-3             -0.34        -0.14        -0.03   \n",
      "3  Lunar Lander  DeepSeek             -0.14        -0.01        -0.01   \n",
      "4  Lunar Lander     GPT-5             -0.23        -0.04        -0.00   \n",
      "5  Lunar Lander   Llama-3             -0.30         0.04        -0.07   \n",
      "\n",
      "   Conciseness  Relevance  Coherence  \n",
      "0         0.01      -0.04      -0.20  \n",
      "1        -0.09       0.00       0.00  \n",
      "2        -0.15      -0.04      -0.06  \n",
      "3        -0.33      -0.00      -0.01  \n",
      "4        -0.13      -0.00      -0.01  \n",
      "5        -0.23      -0.07      -0.07  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fl = pd.read_csv(\"FL_DeepEval.csv\")\n",
    "ll = pd.read_csv(\"LL_DeepEval.csv\")\n",
    "\n",
    "fl[\"Environment\"] = \"Frozen Lake\"\n",
    "ll[\"Environment\"] = \"Lunar Lander\"\n",
    "\n",
    "df = pd.concat([fl, ll], ignore_index=True)\n",
    "\n",
    "metrics = [\n",
    "    \"Prompt Alignment Score\",\n",
    "    \"Correctness Score\",\n",
    "    \"Helpfulness Score\",\n",
    "    \"Conciseness Score\",\n",
    "    \"Relevance Score\",\n",
    "    \"Coherence Score\",\n",
    "]\n",
    "\n",
    "means = (\n",
    "    df.groupby([\"Environment\", \"PromptType\", \"LLM\"])[metrics]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "long_df = means[means[\"PromptType\"] == \"LongDetailed\"]\n",
    "short_df = means[means[\"PromptType\"] == \"ShortSpecific\"]\n",
    "\n",
    "merged = pd.merge(\n",
    "    long_df,\n",
    "    short_df,\n",
    "    on=[\"Environment\", \"LLM\"],\n",
    "    suffixes=(\"_Long\", \"_Short\")\n",
    ")\n",
    "\n",
    "diffs = pd.DataFrame({\n",
    "    \"Environment\": merged[\"Environment\"],\n",
    "    \"LLM\": merged[\"LLM\"],\n",
    "})\n",
    "\n",
    "for metric in metrics:\n",
    "    diffs[metric.replace(\" Score\", \"\")] = (\n",
    "        merged[f\"{metric}_Long\"] - merged[f\"{metric}_Short\"]\n",
    "    )\n",
    "\n",
    "diffs = diffs.round(2)\n",
    "\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1765746768949,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "6x-1pFAdtTtL",
    "outputId": "4562131c-6740-4f79-e747-6ddcb398cda2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Environment       LLM  Prompt Alignment  Correctness  \\\n",
      "0  Obscured vs Original Lunar Lander  DeepSeek              0.07         0.01   \n",
      "1  Obscured vs Original Lunar Lander     GPT-5              0.02        -0.04   \n",
      "2  Obscured vs Original Lunar Lander   Llama-3             -0.14        -0.05   \n",
      "\n",
      "   Helpfulness  Conciseness  Relevance  Coherence  \n",
      "0        -0.00        -0.01      -0.00      -0.00  \n",
      "1        -0.00        -0.01      -0.00      -0.01  \n",
      "2         0.01         0.03       0.02       0.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ll = pd.read_csv(\"LL_DeepEval.csv\")\n",
    "oll = pd.read_csv(\"Obscured_LL_DeepEval.csv\")\n",
    "\n",
    "ll[\"Environment\"] = \"Lunar Lander\"\n",
    "oll[\"Environment\"] = \"Obscured Lunar Lander\"\n",
    "\n",
    "df = pd.concat([ll, oll], ignore_index=True)\n",
    "\n",
    "metrics = [\n",
    "    \"Prompt Alignment Score\",\n",
    "    \"Correctness Score\",\n",
    "    \"Helpfulness Score\",\n",
    "    \"Conciseness Score\",\n",
    "    \"Relevance Score\",\n",
    "    \"Coherence Score\",\n",
    "]\n",
    "\n",
    "means = (\n",
    "    df.groupby([\"Environment\", \"PromptType\", \"LLM\"])[metrics]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "orig = means[\n",
    "    (means[\"Environment\"] == \"Lunar Lander\") &\n",
    "    (means[\"PromptType\"] == \"ShortSpecific\")\n",
    "]\n",
    "\n",
    "obsc = means[\n",
    "    (means[\"Environment\"] == \"Obscured Lunar Lander\") &\n",
    "    (means[\"PromptType\"] == \"ShortSpecificModified\")\n",
    "]\n",
    "\n",
    "merged = pd.merge(\n",
    "    obsc,\n",
    "    orig,\n",
    "    on=\"LLM\",\n",
    "    suffixes=(\"_Obscured\", \"_Original\")\n",
    ")\n",
    "\n",
    "diffs = pd.DataFrame({\n",
    "    \"Environment\": \"Obscured vs Original Lunar Lander\",\n",
    "    \"LLM\": merged[\"LLM\"],\n",
    "})\n",
    "\n",
    "for metric in metrics:\n",
    "    diffs[metric.replace(\" Score\", \"\")] = (\n",
    "        merged[f\"{metric}_Obscured\"] - merged[f\"{metric}_Original\"]\n",
    "    )\n",
    "\n",
    "diffs = diffs.round(2)\n",
    "\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1765752192150,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "cFLdk1H3x49Y",
    "outputId": "c5790dc7-0721-47c6-84b2-69389b812eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Environment     Model                  Metric  \\\n",
      "0                         Frozen Lake     GPT-5  Prompt Alignment Score   \n",
      "1                         Frozen Lake     GPT-5       Correctness Score   \n",
      "2                         Frozen Lake     GPT-5       Helpfulness Score   \n",
      "3                         Frozen Lake     GPT-5       Conciseness Score   \n",
      "4                         Frozen Lake     GPT-5         Relevance Score   \n",
      "5                         Frozen Lake     GPT-5         Coherence Score   \n",
      "6                         Frozen Lake   Llama-3  Prompt Alignment Score   \n",
      "7                         Frozen Lake   Llama-3       Correctness Score   \n",
      "8                         Frozen Lake   Llama-3       Helpfulness Score   \n",
      "9                         Frozen Lake   Llama-3       Conciseness Score   \n",
      "10                        Frozen Lake   Llama-3         Relevance Score   \n",
      "11                        Frozen Lake   Llama-3         Coherence Score   \n",
      "12                        Frozen Lake  DeepSeek  Prompt Alignment Score   \n",
      "13                        Frozen Lake  DeepSeek       Correctness Score   \n",
      "14                        Frozen Lake  DeepSeek       Helpfulness Score   \n",
      "15                        Frozen Lake  DeepSeek       Conciseness Score   \n",
      "16                        Frozen Lake  DeepSeek         Relevance Score   \n",
      "17                        Frozen Lake  DeepSeek         Coherence Score   \n",
      "18                        Frozen Lake  All LLMs  Prompt Alignment Score   \n",
      "19                        Frozen Lake  All LLMs       Correctness Score   \n",
      "20                        Frozen Lake  All LLMs       Helpfulness Score   \n",
      "21                        Frozen Lake  All LLMs       Conciseness Score   \n",
      "22                        Frozen Lake  All LLMs         Relevance Score   \n",
      "23                        Frozen Lake  All LLMs         Coherence Score   \n",
      "24  Obscured vs Original Lunar Lander     GPT-5  Prompt Alignment Score   \n",
      "25  Obscured vs Original Lunar Lander     GPT-5       Correctness Score   \n",
      "26  Obscured vs Original Lunar Lander     GPT-5       Helpfulness Score   \n",
      "27  Obscured vs Original Lunar Lander     GPT-5       Conciseness Score   \n",
      "28  Obscured vs Original Lunar Lander     GPT-5         Relevance Score   \n",
      "29  Obscured vs Original Lunar Lander     GPT-5         Coherence Score   \n",
      "30  Obscured vs Original Lunar Lander   Llama-3  Prompt Alignment Score   \n",
      "31  Obscured vs Original Lunar Lander   Llama-3       Correctness Score   \n",
      "32  Obscured vs Original Lunar Lander   Llama-3       Helpfulness Score   \n",
      "33  Obscured vs Original Lunar Lander   Llama-3       Conciseness Score   \n",
      "34  Obscured vs Original Lunar Lander   Llama-3         Relevance Score   \n",
      "35  Obscured vs Original Lunar Lander   Llama-3         Coherence Score   \n",
      "36  Obscured vs Original Lunar Lander  DeepSeek  Prompt Alignment Score   \n",
      "37  Obscured vs Original Lunar Lander  DeepSeek       Correctness Score   \n",
      "38  Obscured vs Original Lunar Lander  DeepSeek       Helpfulness Score   \n",
      "39  Obscured vs Original Lunar Lander  DeepSeek       Conciseness Score   \n",
      "40  Obscured vs Original Lunar Lander  DeepSeek         Relevance Score   \n",
      "41  Obscured vs Original Lunar Lander  DeepSeek         Coherence Score   \n",
      "42  Obscured vs Original Lunar Lander  All LLMs  Prompt Alignment Score   \n",
      "43  Obscured vs Original Lunar Lander  All LLMs       Correctness Score   \n",
      "44  Obscured vs Original Lunar Lander  All LLMs       Helpfulness Score   \n",
      "45  Obscured vs Original Lunar Lander  All LLMs       Conciseness Score   \n",
      "46  Obscured vs Original Lunar Lander  All LLMs         Relevance Score   \n",
      "47  Obscured vs Original Lunar Lander  All LLMs         Coherence Score   \n",
      "\n",
      "      p-value  \n",
      "0    0.00e+00  \n",
      "1    1.54e-08  \n",
      "2   2.41e-265  \n",
      "3   1.67e-281  \n",
      "4    1.56e-37  \n",
      "5    3.88e-10  \n",
      "6   1.33e-126  \n",
      "7   1.36e-233  \n",
      "8    3.33e-10  \n",
      "9    0.00e+00  \n",
      "10   4.09e-26  \n",
      "11   1.97e-47  \n",
      "12  6.90e-136  \n",
      "13   7.39e-40  \n",
      "14  4.82e-101  \n",
      "15   4.56e-08  \n",
      "16  4.62e-114  \n",
      "17  8.87e-125  \n",
      "18   0.00e+00  \n",
      "19  8.46e-163  \n",
      "20   3.88e-83  \n",
      "21   0.00e+00  \n",
      "22   8.31e-79  \n",
      "23  3.29e-158  \n",
      "24   2.58e-11  \n",
      "25   3.01e-51  \n",
      "26   5.63e-82  \n",
      "27   1.23e-59  \n",
      "28   1.02e-96  \n",
      "29   4.11e-77  \n",
      "30   3.18e-22  \n",
      "31   9.94e-70  \n",
      "32   2.35e-01  \n",
      "33   8.53e-07  \n",
      "34   4.09e-03  \n",
      "35   6.64e-01  \n",
      "36   1.47e-31  \n",
      "37   5.39e-02  \n",
      "38   5.88e-01  \n",
      "39   4.47e-68  \n",
      "40   2.28e-20  \n",
      "41   1.72e-01  \n",
      "42   1.47e-03  \n",
      "43   1.19e-64  \n",
      "44   4.14e-01  \n",
      "45   5.11e-02  \n",
      "46   6.00e-03  \n",
      "47   1.07e-01  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "fl = pd.read_csv(\"FL_DeepEval.csv\")\n",
    "ll = pd.read_csv(\"LL_DeepEval.csv\")\n",
    "oll = pd.read_csv(\"Obscured_LL_DeepEval.csv\")\n",
    "\n",
    "fl[\"Environment\"] = \"Frozen Lake\"\n",
    "ll[\"Environment\"] = \"Lunar Lander\"\n",
    "oll[\"Environment\"] = \"Obscured Lunar Lander\"\n",
    "\n",
    "metrics = [\n",
    "    \"Prompt Alignment Score\",\n",
    "    \"Correctness Score\",\n",
    "    \"Helpfulness Score\",\n",
    "    \"Conciseness Score\",\n",
    "    \"Relevance Score\",\n",
    "    \"Coherence Score\",\n",
    "]\n",
    "\n",
    "def compute_paired_ttests(df1, df2, env_label):\n",
    "    merged = pd.merge(df1, df2, on=\"LLM\", suffixes=(\"_1\", \"_2\"))\n",
    "    results = []\n",
    "\n",
    "    for llm in merged[\"LLM\"].unique():\n",
    "        row = merged[merged[\"LLM\"] == llm]\n",
    "        for metric in metrics:\n",
    "            t_stat, p_val = ttest_rel(row[f\"{metric}_1\"].values, row[f\"{metric}_2\"].values)\n",
    "            results.append([env_label, llm, metric, p_val])\n",
    "\n",
    "    for metric in metrics:\n",
    "        t_stat, p_val = ttest_rel(merged[[f\"{metric}_1\"]].values.flatten(),\n",
    "                                  merged[[f\"{metric}_2\"]].values.flatten())\n",
    "        results.append([env_label, \"All LLMs\", metric, p_val])\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Environment\", \"Model\", \"Metric\", \"p-value\"])\n",
    "\n",
    "fl_long = fl[fl[\"PromptType\"] == \"LongDetailed\"]\n",
    "fl_short = fl[fl[\"PromptType\"] == \"ShortSpecific\"]\n",
    "fl_ttests_df = compute_paired_ttests(fl_long, fl_short, \"Frozen Lake\")\n",
    "\n",
    "ll_short = ll[ll[\"PromptType\"] == \"ShortSpecific\"]\n",
    "oll_short = oll[oll[\"PromptType\"] == \"ShortSpecificModified\"]\n",
    "ll_ttests_df = compute_paired_ttests(oll_short, ll_short, \"Obscured vs Original Lunar Lander\")\n",
    "\n",
    "ttest_results = pd.concat([fl_ttests_df, ll_ttests_df], ignore_index=True)\n",
    "ttest_results[\"p-value\"] = ttest_results[\"p-value\"].apply(lambda x: f\"{x:.2e}\")\n",
    "\n",
    "print(ttest_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5127,
     "status": "ok",
     "timestamp": 1765751617240,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "PYKd4X1z--Gg",
    "outputId": "c50a996e-50d5-4246-b8ba-e4ea99dbfcbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
      "/tmp/ipython-input-3815851502.py:65: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "os.makedirs(\"figures/mean_scores_multi\", exist_ok=True)\n",
    "\n",
    "paths = {\n",
    "    \"FrozenLake\": \"FL_DeepEval.csv\",\n",
    "    \"LunarLander\": \"LL_DeepEval.csv\",\n",
    "    \"LunarLanderObscured\": \"Obscured_LL_DeepEval.csv\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for env_name, filename in paths.items():\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df[\"Environment\"] = env_name\n",
    "        dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df[\"PromptType\"] = df[\"PromptType\"].astype(str).str.strip()\n",
    "df[\"LLM\"] = df[\"LLM\"].astype(str).str.strip()\n",
    "\n",
    "metrics = [\n",
    "    \"Prompt Alignment Score\",\n",
    "    \"Correctness Score\",\n",
    "    \"Helpfulness Score\",\n",
    "    \"Conciseness Score\",\n",
    "    \"Relevance Score\",\n",
    "    \"Coherence Score\"\n",
    "]\n",
    "\n",
    "grouped = df.groupby([\"Environment\", \"PromptType\", \"LLM\"])[metrics]\n",
    "mean_df = grouped.mean().reset_index()\n",
    "sem_df = pd.DataFrame()\n",
    "\n",
    "for env, prompt, llm in mean_df[[\"Environment\",\"PromptType\",\"LLM\"]].values:\n",
    "    subset = df[(df[\"Environment\"]==env) & (df[\"PromptType\"]==prompt) & (df[\"LLM\"]==llm)]\n",
    "    sems = []\n",
    "    for metric in metrics:\n",
    "        if metric == \"Prompt Alignment Score\":\n",
    "            mean_val = subset[metric].mean()\n",
    "            N = len(subset)\n",
    "            sem = np.sqrt(mean_val*(1-mean_val)/N)\n",
    "        else:\n",
    "            sem = subset[metric].std(ddof=1)/np.sqrt(len(subset))\n",
    "        sems.append(sem)\n",
    "    sem_df = pd.concat([sem_df, pd.DataFrame([[env,prompt,llm]+sems], columns=[\"Environment\",\"PromptType\",\"LLM\"]+metrics)], ignore_index=True)\n",
    "\n",
    "melted_mean = mean_df.melt(id_vars=[\"Environment\",\"PromptType\",\"LLM\"], value_vars=metrics, var_name=\"Metric\", value_name=\"MeanScore\")\n",
    "melted_sem = sem_df.melt(id_vars=[\"Environment\",\"PromptType\",\"LLM\"], value_vars=metrics, var_name=\"Metric\", value_name=\"SEM\")\n",
    "plot_df = melted_mean.merge(melted_sem, on=[\"Environment\",\"PromptType\",\"LLM\",\"Metric\"])\n",
    "\n",
    "for env in plot_df[\"Environment\"].unique():\n",
    "    sub = plot_df[plot_df[\"Environment\"]==env]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        metric_sub = sub[sub[\"Metric\"]==metric]\n",
    "        sns.barplot(data=metric_sub, x=\"LLM\", y=\"MeanScore\", hue=\"PromptType\", ci=None, ax=ax)\n",
    "        for j, row in metric_sub.iterrows():\n",
    "            x_positions = list(metric_sub[\"LLM\"].unique())\n",
    "            x = x_positions.index(row[\"LLM\"]) + (0 if row[\"PromptType\"] in [\"ShortSpecific\",\"ShortSpecificModified\"] else 0.2)\n",
    "            ax.errorbar(x, row[\"MeanScore\"], yerr=row[\"SEM\"], fmt=\"none\", c='black', capsize=4)\n",
    "        ax.set_title(metric, fontsize=14)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Mean Score\", fontsize=12)\n",
    "        ax.tick_params(axis='x', labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, title=\"\", loc=\"upper left\", ncol=2, fontsize=12)\n",
    "    fig.suptitle(f\"{env} — Mean Scores by LLM and Prompt Type\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0,0,1,0.95])\n",
    "    safe_env = env.replace(\" \", \"_\")\n",
    "    plt.savefig(f\"figures/mean_scores_multi/{safe_env}_all_metrics.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2848,
     "status": "ok",
     "timestamp": 1765785267214,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "rqDqjOzfAKQk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "os.makedirs(\"figures/correlation\", exist_ok=True)\n",
    "\n",
    "metrics = [\n",
    "    \"Prompt Alignment Score\",\n",
    "    \"Correctness Score\",\n",
    "    \"Helpfulness Score\",\n",
    "    \"Conciseness Score\",\n",
    "    \"Relevance Score\",\n",
    "    \"Coherence Score\"\n",
    "]\n",
    "\n",
    "orig = pd.read_csv(\"LL_DeepEval.csv\")\n",
    "orig[\"PromptType\"] = orig[\"PromptType\"].astype(str).str.strip()\n",
    "orig[\"LLM\"] = orig[\"LLM\"].astype(str).str.strip()\n",
    "\n",
    "orig_corr = orig[metrics].corr()\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "    orig_corr,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"vlag\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar_kws={\"label\": \"Correlation\"}\n",
    ")\n",
    "plt.title(\"Original Lunar Lander — Metric Correlation Matrix\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/correlation/original_lunarlander_corr.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "obs = pd.read_csv(\"Obscured_LL_DeepEval.csv\")\n",
    "obs[\"PromptType\"] = obs[\"PromptType\"].astype(str).str.strip()\n",
    "obs[\"LLM\"] = obs[\"LLM\"].astype(str).str.strip()\n",
    "\n",
    "obs_corr = obs[metrics].corr()\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "    obs_corr,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"vlag\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar_kws={\"label\": \"Correlation\"}\n",
    ")\n",
    "plt.title(\"Obscured Lunar Lander — Metric Correlation Matrix\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/correlation/obscured_lunarlander_corr.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1765788554097,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "ZKDzbitiMmxh",
    "outputId": "c2fe1408-a231-4526-d0ae-6783e4bec855"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-400135631.py:40: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "os.makedirs(\"figures/prompt_alignment\", exist_ok=True)\n",
    "\n",
    "files = {\n",
    "    \"Frozen Lake\": \"FL_DeepEval.csv\",\n",
    "    \"Original Lunar Lander\": \"LL_DeepEval.csv\",\n",
    "    \"Obscured Lunar Lander\": \"Obscured_LL_DeepEval.csv\"\n",
    "}\n",
    "\n",
    "metric = \"Prompt Alignment Score\"\n",
    "all_data = []\n",
    "\n",
    "for env, path in files.items():\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    df[\"PromptType\"] = df[\"PromptType\"].astype(str).str.strip()\n",
    "    df[\"LLM\"] = df[\"LLM\"].astype(str).str.strip()\n",
    "\n",
    "    grouped = df.groupby([\"LLM\", \"PromptType\"])[metric]\n",
    "    means = grouped.mean()\n",
    "    ses = grouped.apply(lambda x: np.sqrt(x.mean() * (1 - x.mean()) / len(x)))\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"Mean\": means,\n",
    "        \"SE\": ses,\n",
    "        \"Environment\": env\n",
    "    }).reset_index()\n",
    "\n",
    "    all_data.append(summary)\n",
    "\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=df_all,\n",
    "    x=\"Environment\",\n",
    "    y=\"Mean\",\n",
    "    hue=\"PromptType\",\n",
    "    ci=None,\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "\n",
    "for i, row in df_all.iterrows():\n",
    "    env_idx = list(df_all[\"Environment\"].unique()).index(row[\"Environment\"])\n",
    "    prompt_types = df_all[\"PromptType\"].unique()\n",
    "    hue_idx = list(prompt_types).index(row[\"PromptType\"])\n",
    "    total_prompts = len(prompt_types)\n",
    "    offset = (hue_idx - total_prompts/2) * 0.2 + 0.1\n",
    "    plt.errorbar(\n",
    "        x=env_idx + offset,\n",
    "        y=row[\"Mean\"],\n",
    "        yerr=row[\"SE\"],\n",
    "        fmt='none',\n",
    "        c='black',\n",
    "        capsize=4\n",
    "    )\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Mean Prompt Alignment\")\n",
    "plt.title(\"Prompt Alignment Across Environments, LLMs, and Prompt Types\")\n",
    "plt.legend(title=\"Prompt Type\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/prompt_alignment/all_environments_prompt_alignment.png\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1859,
     "status": "ok",
     "timestamp": 1765788772318,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "P1PjuPObNjXd",
    "outputId": "8632d48a-0589-4c65-9434-c105a5cc5e64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3747307186.py:40: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
      "\n",
      "  g = sns.catplot(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "os.makedirs(\"figures/prompt_alignment\", exist_ok=True)\n",
    "\n",
    "files = {\n",
    "    \"Frozen Lake\": \"FL_DeepEval.csv\",\n",
    "    \"Original Lunar Lander\": \"LL_DeepEval.csv\",\n",
    "    \"Obscured Lunar Lander\": \"Obscured_LL_DeepEval.csv\"\n",
    "}\n",
    "\n",
    "metric = \"Prompt Alignment Score\"\n",
    "all_data = []\n",
    "\n",
    "for env, path in files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"PromptType\"] = df[\"PromptType\"].astype(str).str.strip()\n",
    "    df[\"LLM\"] = df[\"LLM\"].astype(str).str.strip()\n",
    "\n",
    "    grouped = df.groupby([\"LLM\", \"PromptType\"])[metric]\n",
    "    means = grouped.mean()\n",
    "    ses = grouped.apply(lambda x: np.sqrt(x.mean() * (1 - x.mean()) / len(x)))\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"Mean\": means,\n",
    "        \"SE\": ses,\n",
    "        \"Environment\": env\n",
    "    }).reset_index()\n",
    "\n",
    "    all_data.append(summary)\n",
    "\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_all,\n",
    "    x=\"LLM\",\n",
    "    y=\"Mean\",\n",
    "    hue=\"PromptType\",\n",
    "    col=\"Environment\",\n",
    "    kind=\"bar\",\n",
    "    palette=\"Set2\",\n",
    "    ci=None,\n",
    "    height=5,\n",
    "    aspect=1\n",
    ")\n",
    "\n",
    "for ax, env in zip(g.axes.flat, df_all[\"Environment\"].unique()):\n",
    "    env_data = df_all[df_all[\"Environment\"] == env]\n",
    "    for i, row in env_data.iterrows():\n",
    "        llm_idx = list(env_data[\"LLM\"].unique()).index(row[\"LLM\"])\n",
    "        prompt_idx = list(env_data[\"PromptType\"].unique()).index(row[\"PromptType\"])\n",
    "        total_prompts = len(env_data[\"PromptType\"].unique())\n",
    "        offset = (prompt_idx - total_prompts / 2) * 0.8 / total_prompts + 0.4 / total_prompts\n",
    "        ax.errorbar(\n",
    "            x=llm_idx + offset,\n",
    "            y=row[\"Mean\"],\n",
    "            yerr=row[\"SE\"],\n",
    "            fmt='none',\n",
    "            c='black',\n",
    "            capsize=4\n",
    "        )\n",
    "\n",
    "g.set(ylim=(0, 1))\n",
    "g.set_axis_labels(\"LLM\", \"Mean Prompt Alignment\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.fig.suptitle(\"Prompt Alignment Across Environments, LLMs, and Prompt Types\", y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/prompt_alignment/all_environments_prompt_alignment.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1765795564986,
     "user": {
      "displayName": "Nivetha",
      "userId": "07905950540777022623"
     },
     "user_tz": -60
    },
    "id": "FfX-VAZQnUTV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = {\n",
    "    \"Frozen Lake\": \"FL_DeepEval.csv\",\n",
    "    \"Original Lunar Lander\": \"LL_DeepEval.csv\",\n",
    "    \"Obscured Lunar Lander\": \"Obscured_LL_DeepEval.csv\"\n",
    "}\n",
    "\n",
    "bands = {\n",
    "    \"Low (0--0.2)\": (0.0, 0.2),\n",
    "    \"Medium (0.3--0.5)\": (0.3, 0.5),\n",
    "    \"High (0.8--1.0)\": (0.8, 1.0)\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    \"Prompt Alignment\": \"Prompt Alignment Score\",\n",
    "    \"Correctness\": \"Correctness Score\",\n",
    "    \"Helpfulness\": \"Helpfulness Score\",\n",
    "    \"Conciseness\": \"Conciseness Score\",\n",
    "    \"Relevance\": \"Relevance Score\",\n",
    "    \"Coherence\": \"Coherence Score\"\n",
    "}\n",
    "\n",
    "def in_band(s, lo, hi):\n",
    "    return (s >= lo) & (s <= hi)\n",
    "\n",
    "def escape_latex(t):\n",
    "    return (\n",
    "        str(t)\n",
    "        .replace(\"\\\\\", \"\\\\textbackslash{}\")\n",
    "        .replace(\"&\", \"\\\\&\")\n",
    "        .replace(\"%\", \"\\\\%\")\n",
    "        .replace(\"$\", \"\\\\$\")\n",
    "        .replace(\"#\", \"\\\\#\")\n",
    "        .replace(\"_\", \"\\\\_\")\n",
    "        .replace(\"{\", \"\\\\{\")\n",
    "        .replace(\"}\", \"\\\\}\")\n",
    "        .replace(\"~\", \"\\\\textasciitilde{}\")\n",
    "        .replace(\"^\", \"\\\\textasciicircum{}\")\n",
    "    )\n",
    "\n",
    "with open(\"appendix_qualitative_examples.tex\", \"w\") as f:\n",
    "    f.write(\"\\\\section{Automatically Selected Qualitative Examples}\\n\")\n",
    "    f.write(\"\\\\label{app:qualitative_examples}\\n\")\n",
    "\n",
    "    for env, path in files.items():\n",
    "        df = pd.read_csv(path)\n",
    "        df[\"LLM\"] = df[\"LLM\"].astype(str).str.strip()\n",
    "        df[\"PromptType\"] = df[\"PromptType\"].astype(str).str.strip()\n",
    "\n",
    "        f.write(f\"\\\\subsection{{{env}}}\\n\")\n",
    "\n",
    "        for metric_name, col in metrics.items():\n",
    "\n",
    "            if metric_name == \"Prompt Alignment\":\n",
    "                selections = [(0.0, \"0\"), (1.0, \"1\")]\n",
    "            else:\n",
    "                selections = bands.items()\n",
    "\n",
    "            for label, rng in selections:\n",
    "\n",
    "                if metric_name == \"Prompt Alignment\":\n",
    "                    subset = df[df[col] == label]\n",
    "                    title = f\"{metric_name} = {rng}\"\n",
    "                else:\n",
    "                    lo, hi = rng\n",
    "                    subset = df[in_band(df[col], lo, hi)]\n",
    "                    title = f\"{metric_name} -- {label}\"\n",
    "\n",
    "                if subset.empty:\n",
    "                    continue\n",
    "\n",
    "                r = subset.iloc[0]\n",
    "                text = escape_latex(r[\"Explanation\"])\n",
    "\n",
    "                f.write(rf\"\"\"\n",
    "\\begin{{tcolorbox}}[\n",
    "    colback=gray!5,\n",
    "    colframe=black,\n",
    "    width=\\linewidth,\n",
    "    breakable,\n",
    "    enhanced,\n",
    "    boxrule=0.6pt\n",
    "]\n",
    "\\justifying\n",
    "\\tiny\n",
    "\\textbf{{LLM:}} {r[\"LLM\"]} \\\\\n",
    "\\textbf{{Prompt Type:}} {r[\"PromptType\"]} \\\\[2pt]\n",
    "\\textbf{{Metric:}} {title} \\\\[2pt]\n",
    "\\textit{{Explanation:}} ``{text}''\n",
    "\\end{{tcolorbox}}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMqPQJHf4FWJrjWVuDtwhQF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
